{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPavlov中的数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一部分将学习如何为训练组件读取和准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepPavlov具有下载和解压数据的功能，因此需要调用 data.utils 的 download_depress 包。下面的部分代码的作用是从 NER(命名实体识别)任务中下载 CoNLL-2003 数据集，并把它存放在 data/ 文件夹中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把下载好的文本数据解析成机器可读的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们将运用到一个包含带有命名实体标签的推特文章的语料库(corpus)。一个典型的命名实体识别数据文件包含符号（*tokens*）（词或标点符号）和标签（*tags*），它们被空格分隔开。有的时候一些附加信息，比如 POS-tags 也是包含在其中的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的文件是用 **-DOCSTART-** 开头的一行分隔开的，不同的句子是用一行空白行分隔开的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如：\n",
    "  \n",
    "  -DOCSTART- -X- -X- O\n",
    "\n",
    "  EU NNP B-NP B-ORG  \n",
    "  rejects VBZ B-VP O  \n",
    "  German JJ B-NP B-MISC  \n",
    "  call NN I-NP O  \n",
    "  to TO B-VP O  \n",
    "  boycott VB I-VP O  \n",
    "  British JJ B-NP B-MISC  \n",
    "  lamb NN I-NP O  \n",
    "  . . O O  \n",
    "\n",
    "  Peter NNP B-NP B-PER  \n",
    "  Blackburn NNP I-NP I-PER  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个教程中我们只关注tokens和tags（也就是每行的第一个元素和最后一个元素）,而忽略掉两者之间的POS元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先新建一个NerDatasetReader类，用来读取数据集。它返回的是一个dictionary包含train,test,valid这三个field，每个field存储着一些sample构成的list，每个sample是由tokens和tags构成的tuple,其中tokens和tags是list。  \n",
    "下面的例子描述了这个dictionary的结构，它由NerDatasetReader类中的read()方法返回："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "{'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    " 'valid': [...],\n",
    " 'test': [...]}\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集分为三个部分：  \n",
    "1.train: 用来训练模型  \n",
    "2.valid: 用来评估以及参数调优  \n",
    "3.test:用来最终评估模型  \n",
    "这三个部分分别存在三个txt文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "from pathlib import Path\n",
    "\n",
    "class NerDatasetReader:\n",
    "    def read(self, data_path):\n",
    "        data_paths = ['train', 'valid', 'test']\n",
    "        extension = '.txt'\n",
    "        dataset ={}\n",
    "        for data_part in data_parts:\n",
    "            file_path = Path(data_path) / Path(data_path + extension)\n",
    "            dataset[data_part] = self.read_file(str(file_path))\n",
    "        return dataset\n",
    "    def read_file(file_path):\n",
    "        \n",
    "        # Use utf-8 encoding when open the file\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        ######################################\n",
    "        return samples\n",
    "\n",
    "dataset_reader = NerDatasetReader()\n",
    "dataset = dataset_reader.read('data/')\n",
    "assert len(dataset) == 3, 'The dataset must be a dict with three fields: train, test, and valid'\n",
    "assert len(set(dataset) & {'train', 'test', 'valid'}) == 3, 'The dataset keys must be exactly train, test, and valid'\n",
    "assert isinstance(dataset['train'][0][0][0], str) and isinstance(dataset['train'][0][0][1], str), 'Both tokens and tags must be strings'\n",
    "assert len(dataset['train']) == 14041, 'there must be exactly 14041 samples in train'\n",
    "assert len(dataset['valid']) == 3250, 'there must be exactly 3250 samples in train'\n",
    "assert len(dataset['test']) == 3453, 'there must be exactly 3453 samples in test'\n",
    "        \n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "\n",
    "\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
