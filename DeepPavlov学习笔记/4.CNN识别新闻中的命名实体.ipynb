{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  用CNN(卷积神经网络)识别新闻数据中的命名实体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*在这个教程中，我们会用卷积神经网络（convolutional neural network,CNN）去解决命名实体识别（Named Entity Recognition,NER）的问题。*\n",
    "  \n",
    "*命名实体识别是自然语言处理中经常遇到的问题，它的作用是从文本中抽取出一些实体，例如人、机构、地点等等。* \n",
    "  \n",
    "*在这里，我们会做些实验，在CoNLL-2003数据集中的不同新闻中，识别出命名实体。*  \n",
    "  \n",
    "  例如，我们想从下面这句话中解析出人和机构的名字\n",
    ">Yan Goodfellow works for Google Brain\n",
    "  \n",
    "  NER模型需要提供如下的标签(tags)序列:\n",
    ">B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "  \n",
    "  \n",
    "  这里有两个前缀：  \n",
    "*B-*代表着实体的beginning  \n",
    "*I-*代表着实体的inside  \n",
    "*O*代表没有标签  \n",
    "带有这种前缀的标记称为BIO标记(BIO markup),引入此标记是为了区分具有相似类型的后续实体。  \n",
    "  \n",
    "  \n",
    "  解决这种问题需要用到神经网络的相关知识，尤其是**卷积神经网络**。  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "下面的单元格将把这个任务所需的所有数据下载到文件夹 \\data 中，库中的下载工具用来下载和提取文件的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:28:32.227 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "2018-08-20 17:28:32.529 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 302 None\n",
      "2018-08-20 17:28:32.541 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): 202.112.144.234:80\n",
      "2018-08-20 17:28:32.599 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://202.112.144.234:80 \"GET /files/3146000001CF3D15/lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 200 957092\n",
      "2018-08-20 17:28:32.601 INFO in 'deeppavlov.core.data.utils'['utils'] at line 62: Downloading from http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz to data/conll2003_v2.tar.gz\n",
      "100%|██████████| 957k/957k [00:00<00:00, 14.3MB/s]\n",
      "2018-08-20 17:28:32.691 INFO in 'deeppavlov.core.data.utils'['utils'] at line 200: Extracting data/conll2003_v2.tar.gz archive into data\n"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载CoNLL-2003命名实体识别语料库\n",
    "这里我们将运用到一个包含带有命名实体标签的推特文章的语料库(corpus)。一个典型的命名实体识别数据文件包含符号（*tokens*）（词或标点符号）和标签（*tags*），它们被空格分隔开。有的时候一些附加信息，比如 POS-tags 也是包含在其中的。\n",
    "不同的文件是用 **-DOCSTART-** 开头的一行分隔开的，不同的句子是用一行空白行分隔开的。\n",
    "例如：\n",
    "~~~\n",
    "  \n",
    "  -DOCSTART- -X- -X- O\n",
    "\n",
    "  EU NNP B-NP B-ORG  \n",
    "  rejects VBZ B-VP O  \n",
    "  German JJ B-NP B-MISC  \n",
    "  call NN I-NP O  \n",
    "  to TO B-VP O  \n",
    "  boycott VB I-VP O  \n",
    "  British JJ B-NP B-MISC  \n",
    "  lamb NN I-NP O  \n",
    "  . . O O  \n",
    "\n",
    "  Peter NNP B-NP B-PER  \n",
    "  Blackburn NNP I-NP I-PER  \n",
    "  \n",
    "~~~\n",
    "这个教程中我们只关注tokens和tags（也就是每行的第一个元素和最后一个元素）,而忽略掉两者之间的POS元素。  \n",
    "  \n",
    "\n",
    "我们先新建一个Conll2003DatasetReader类，用来读取数据集。它返回的是一个dictionary包含train,test,valid这三个field，每个field存储着一些sample构成的list，每个sample是由tokens和tags构成的tuple,其中tokens和tags是list。\n",
    "下面的例子描述了这个dictionary的结构，它由NerDatasetReader类中的read()方法返回：  \n",
    "~~~\n",
    "{'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    " 'valid': [...],\n",
    " 'test': [...]}\n",
    "\n",
    "~~~  \n",
    "数据集分为三个部分：  \n",
    "1.train: 用来训练模型  \n",
    "2.valid: 用来评估以及参数调优  \n",
    "3.test:用来最终评估模型  \n",
    "这三个部分分别存在三个txt文件中。  \n",
    "  \n",
    "我们会用库中的Conll2003DatasetReader类来读取数据，也就是把文本转换成如上所说的形式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应该始终了解我们处理的数据类型，因此，我们用下面的代码把它们打印出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOCSTART>\tO\n",
      "\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n",
      "The\tO\n",
      "European\tB-ORG\n",
      "Commission\tI-ORG\n",
      "said\tO\n",
      "on\tO\n",
      "Thursday\tO\n",
      "it\tO\n",
      "disagreed\tO\n",
      "with\tO\n",
      "German\tB-MISC\n",
      "advice\tO\n",
      "to\tO\n",
      "consumers\tO\n",
      "to\tO\n",
      "shun\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      "until\tO\n",
      "scientists\tO\n",
      "determine\tO\n",
      "whether\tO\n",
      "mad\tO\n",
      "cow\tO\n",
      "disease\tO\n",
      "can\tO\n",
      "be\tO\n",
      "transmitted\tO\n",
      "to\tO\n",
      "sheep\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset['train'][:5]:\n",
    "    for token,tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token,tag))\n",
    "    print()\n",
    "    # zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n",
    "    # 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备字典\n",
    "为了训练出来一个神经网络，我们需要用到两个映射（mapping）：  \n",
    "* {token}$\\to${token id}: 为当前的token处理嵌入矩阵中的行\n",
    "* {tag}$\\to${tag id}:制造one-hot地面真值概率分布向量，用于计算网络输出损耗。  \n",
    "  \n",
    "库中的 SimpleVocabulary 将会执行这些映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将会为token和tag准备字典。有时词汇表中有一些特殊的token，例如一个未知的单词标记，每当我们遇到词汇表之外的单词时就会使用它。这种情况下，我们就会用< UNK > 这种特殊的记号来表示词汇表之外的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:28:33.33 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2018-08-20 17:28:33.35 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们在数据的训练部分中加入词汇表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]# 这是list\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试得到索引，请记住，我们正在使用以下结构的批次:\n",
    "~~~\n",
    "[['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10167, 6, 168, 7, 6097, 5518, 1865]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([['How', 'to', 'do', 'a', 'barrel', 'roll', '?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [3, 5]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['O', 'O', 'O'], ['B-ORG', 'I-ORG']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['I-ORG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们试试从索引到token的转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['31',\n",
       "  '10',\n",
       "  'go',\n",
       "  'loss',\n",
       "  'minutes',\n",
       "  'so',\n",
       "  'matches',\n",
       "  'then',\n",
       "  'for',\n",
       "  'following']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_vocab([np.random.randint(0, 512, size=10)])\n",
    "# numpy.random.randint()用法\n",
    "# numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "# low : int 产生随机数的最小值\n",
    "# high : int, optional 给随机数设置个上限，即产生的随机数必须小于high\n",
    "# size : int or tuple of ints, optional 输出的大小，可以是整数，或者元组\n",
    "# dtype : dtype, optional 期望结果的类型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', ',']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([[1,2]])# 索引1、2分别对应着'.'和‘,’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集迭代器(Iterator)\n",
    "神经网络通常是分批训练的。这意味着网络的权值更新是基于每一次的多个序列。每一批中的所有序列需要具有相同的长度。因此，我们将向它们填充一个特殊的< UKN >记号。同样，token和tag也必须填充它。为循环神经网络(Recurrent Neural Network，RNN)提供序列长度是很好的实践，所以它可以跳过对填充部件的计算。我们在这里供批处理函数batches_generator以节省时间。  \n",
    "  \n",
    "  批量生成的一个重要概念是打乱(shuffling)。打乱是从数据集中随机抽取样本,对打乱后的数据进行训练是很重要的，因为从同一类抽取的大量结果样本可能导致模型太过于”纯净“。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从加载的数据集中创建数据集迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['Algeria',\n",
       "   ',',\n",
       "   'fighting',\n",
       "   'a',\n",
       "   'vicious',\n",
       "   'war',\n",
       "   'against',\n",
       "   'Moslem',\n",
       "   'fundamentalist',\n",
       "   'guerrillas',\n",
       "   ',',\n",
       "   'attacked',\n",
       "   'Britain',\n",
       "   'on',\n",
       "   'Wednesday',\n",
       "   'for',\n",
       "   'allowing',\n",
       "   'Islamist',\n",
       "   'groups',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'in',\n",
       "   'London',\n",
       "   '.'],\n",
       "  ['Perot',\n",
       "   'won',\n",
       "   'his',\n",
       "   'party',\n",
       "   \"'s\",\n",
       "   'official',\n",
       "   'nomination',\n",
       "   'as',\n",
       "   'its',\n",
       "   'presidential',\n",
       "   'candidate',\n",
       "   'in',\n",
       "   'a',\n",
       "   'secret',\n",
       "   'ballot',\n",
       "   'earlier',\n",
       "   'this',\n",
       "   'month',\n",
       "   '.']),\n",
       " (['B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O'],\n",
       "  ['B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(2, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['The',\n",
       "   'Egyptian',\n",
       "   'government',\n",
       "   'will',\n",
       "   'have',\n",
       "   'nothing',\n",
       "   'more',\n",
       "   'to',\n",
       "   'do',\n",
       "   'with',\n",
       "   'the',\n",
       "   'Sudanese',\n",
       "   'government',\n",
       "   'because',\n",
       "   'it',\n",
       "   'continues',\n",
       "   'to',\n",
       "   'shelter',\n",
       "   'and',\n",
       "   'support',\n",
       "   'Egyptian',\n",
       "   'militants',\n",
       "   ',',\n",
       "   'President',\n",
       "   'Hosni',\n",
       "   'Mubarak',\n",
       "   'said',\n",
       "   'in',\n",
       "   'a',\n",
       "   'speech',\n",
       "   'on',\n",
       "   'Thursday',\n",
       "   '.'],\n",
       "  ['Glickman', 'says', 'USDA', 'monitoring', 'aflatoxin', 'in', 'Texas', '.'],\n",
       "  ['Although',\n",
       "   'Christie',\n",
       "   ',',\n",
       "   'who',\n",
       "   'is',\n",
       "   'not',\n",
       "   'racing',\n",
       "   'the',\n",
       "   'individual',\n",
       "   '100',\n",
       "   'metres',\n",
       "   'in',\n",
       "   'Berlin',\n",
       "   ',',\n",
       "   'took',\n",
       "   'his',\n",
       "   'time',\n",
       "   'to',\n",
       "   'agree',\n",
       "   'to',\n",
       "   'run',\n",
       "   ',',\n",
       "   'the',\n",
       "   'veteran',\n",
       "   'was',\n",
       "   'clearly',\n",
       "   'delighted',\n",
       "   'to',\n",
       "   'be',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'tribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'black',\n",
       "   'American',\n",
       "   '.'],\n",
       "  ['SepOct', '733.75', '743.50', 'unq', 'unq'],\n",
       "  ['Extras', '(', 'nb-5', ')', '5']),\n",
       " (['O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'I-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['B-PER', 'O', 'B-ORG', 'O', 'O', 'O', 'B-LOC', 'O'],\n",
       "  ['O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O']))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(5, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成掩码\n",
    "关于生成训练数据的最后一件事。我们需要生成一个二进制掩码，在这个掩码中，token代表1，其他代表是0。  \n",
    "这个掩码将阻止通过填充来反向传播。  \n",
    "此类掩码的一个实例:\n",
    "~~~\n",
    "[[1, 1, 0, 0, 0],\n",
    " [1, 1, 1, 1, 1]]\n",
    "~~~\n",
    "代表这些句子：\n",
    "~~~\n",
    " [['The', 'roof'],\n",
    "  ['This', 'is', 'my', 'domain', '!']]\n",
    "~~~\n",
    "掩码长度必须等于批次中句子的最大长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "get_mask = Mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立一个循环神经网络(RNN)\n",
    "这是任务最重要的部分，在这里，我们将指定基于TensorFlow构建块的网络体系结构。  \n",
    "我们将创建一个卷积神经网络（CNN），它将为句子中的每个token生成tag的概率分布。为了考虑token的右侧和左侧上下文，我们将使用CNN。将在顶部使用全连接层(Dense Layer)来执行标签分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在NLP领域中，几乎所有网络的一个重要组成部分就是词语的嵌入。我们将文本作为一系列token传递给网络。每个token都由其索引表示。对于每个token(索引)我们有一个向量。总的来说，这些向量构成了一个嵌入矩阵。这个矩阵可以使用像Skip-Gram或CBOW这样的通用算法进行预训练，也可以由随机值初始化并与网络的其他参数一起训练。在本教程中，我们将遵循**第二种选择**。  \n",
    "\n",
    "我们需要构建一个函数，它使用形状为\\[batch_size, num_token\\]的token索引张量，对于这个矩阵中的每个索引，它从嵌入矩阵中检索一个与该索引对应的向量。这就产生了一个新的张量\\[batch_size, num_token, emb_dim\\]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(indices, vocabulary_size, emb_dim):\n",
    "    # Initialize the random gaussian matrix with dimensions [vocabulary_size, embedding_dimension]\n",
    "    # The **VARIANCE** of the random samples must be 1 / embedding_dimension\n",
    "    emb_mat = np.random.randn(vocabulary_size, emb_dim).astype(np.float32) / np.sqrt(emb_dim) # YOUR CODE HERE\n",
    "    emb_mat = tf.Variable(emb_mat, name='Embeddings', trainable=True)\n",
    "    emb = tf.nn.embedding_lookup(emb_mat, indices)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的主体是卷积层。卷积背后的基本思想是对每个连续的n个示例(在我们的例子中是token)应用相同的全连接层(Dense Layer)。下面描述了一个简化的例子:  \n",
    "![](https://github.com/deepmipt/DeepPavlov/raw/c7896c6db96f43f57cacd9a6a471e37cb70bf07a/examples/tutorials/img/convolution.png)  \n",
    "这里的输入和输出特征数等于1。  \n",
    "让我们以一个简单的例子用它：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d/BiasAdd:0\", shape=(2, 3, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with shape [batch_size, number_of_tokens, number_of_features]\n",
    "x = tf.random_normal(shape=[2, 10, 100])\n",
    "y = tf.layers.conv1d(x, filters=200, kernel_size=8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您所看到的，由于零填充(输入的开头和结尾都是0)的缺失，number_of_tokens维上的结果张量的大小减小了。  \n",
    "要使用填充并保持沿卷积维数的维数，需要向函数传递padding='same'参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_1/BiasAdd:0\", shape=(2, 10, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_with_padding = tf.layers.conv1d(x, filters=200, kernel_size=8, padding='same')\n",
    "print(y_with_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在用n_hidden_list变量给的维数来堆叠一些层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(units, n_hidden_list, cnn_filter_width, activation=tf.nn.relu):\n",
    "    # Use activation(units) to apply activation to units\n",
    "    for n_hidden in n_hidden_list:\n",
    "        \n",
    "        units = tf.layers.conv1d(units,\n",
    "                                 n_hidden,\n",
    "                                 cnn_filter_width,\n",
    "                                 padding='same')\n",
    "        units = activation(units)\n",
    "    return units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务的一个常见损失是交叉熵。为什么要分类?因为对于每个标记，网络必须决定预测哪个标记。交叉熵的形式如下所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://render.githubusercontent.com/render/math?math=H%28P%2C%20Q%29%20%3D%20-E_%7Bx%20%5Csim%20P%7D%20log%20Q%28x%29&mode=display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它衡量了真值分布与预测分布之间的差异。在大多数情况下，真值分布是one-hot的。幸运的是，这种损失已经在TensorFlow中实现了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_cross_entropy_with_logits/Reshape_2:0\", shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The logits\n",
    "l = tf.random_normal([1, 4, 3]) # shape [batch_size, number_of_tokens, number of classes]\n",
    "indices = tf.placeholder(tf.int32, [1, 4])\n",
    "\n",
    "# Make one-hot distribution from indices for 3 types of tag\n",
    "p = tf.one_hot(indices, depth=3)\n",
    "loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=p, logits=l)\n",
    "print(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一批句子都有相同的长度，我们把每个句子填充成最长。所以在末端有填补，并且推动网络去预测那些填补通常会导致质量恶化。然后，我们需要用二进制掩码乘以损失张量，以防止从填补中产生梯度流动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.placeholder(tf.float32, shape=[1, 4])\n",
    "loss_tensor *= mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是计算损失张量的平均值:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在定义一个函数来返回一个标量掩蔽交叉熵损失（scalar masked cross-entropy loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, label_indices, number_of_tags, mask):\n",
    "    ground_truth_labels = tf.one_hot(label_indices, depth=number_of_tags)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_labels, logits=logits)\n",
    "    loss_tensor *= mask\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来把所有东西都放到一个类里面去："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy  as  np\n",
    "import  tensorflow  as tf\n",
    "\n",
    "class NerNetwork:\n",
    "    def __init__(self,\n",
    "                 n_tokens,\n",
    "                 n_tags,\n",
    "                 token_emb_dim=100,\n",
    "                 n_hidden_list=(128,),\n",
    "                 cnn_filter_width=7,\n",
    "                 use_batch_norm=False,\n",
    "                 embeddings_dropout=False,\n",
    "                 top_dropout=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # ================ Building inputs =================\n",
    "        \n",
    "        self.learning_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.dropout_keep_ph = tf.placeholder(tf.float32, [])\n",
    "        self.token_ph = tf.placeholder(tf.int32, [None, None], name='token_ind_ph')\n",
    "        self.mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "        self.y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "        \n",
    "        # ================== Building the network ==================\n",
    "        \n",
    "        # Now embedd the indices of tokens using token_emb_dim function\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        emb = get_embeddings(self.token_ph, n_tokens, token_emb_dim)\n",
    "        ######################################\n",
    "\n",
    "        emb = tf.nn.dropout(emb, self.dropout_keep_ph, (tf.shape(emb)[0], 1, tf.shape(emb)[2]))\n",
    "        \n",
    "        # Build a multilayer CNN on top of the embeddings.\n",
    "        # The number of units in the each layer must match\n",
    "        # corresponding number from n_hidden_list.\n",
    "        # Use ReLU activation \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        units = conv_net(emb, n_hidden_list, cnn_filter_width)\n",
    "        ######################################\n",
    "        units = tf.nn.dropout(units, self.dropout_keep_ph, (tf.shape(units)[0], 1, tf.shape(units)[2]))\n",
    "        logits = tf.layers.dense(units, n_tags, activation=None)\n",
    "        self.predictions = tf.argmax(logits, 2)\n",
    "        \n",
    "        # ================= Loss and train ops =================\n",
    "        # Use cross-entropy loss. check the tf.nn.softmax_cross_entropy_with_logits_v2 function\n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        self.loss = masked_cross_entropy(logits, self.y_ph, n_tags, self.mask_ph)\n",
    "        ######################################\n",
    "\n",
    "        # Create a training operation to update the network parameters.\n",
    "        # We purpose to use the Adam optimizer as it work fine for the\n",
    "        # most of the cases. Check tf.train to find an implementation.\n",
    "        # Put the train operation to the attribute self.train_op\n",
    "        \n",
    "        ######################################\n",
    "        ########## YOUR CODE HERE ############\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        ######################################\n",
    "\n",
    "        # ================= Initialize the session =================\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def __call__(self, tok_batch, mask_batch):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: 1.0}\n",
    "        return self.sess.run(self.predictions, feed_dict)\n",
    "\n",
    "    def train_on_batch(self, tok_batch, tag_batch, mask_batch, dropout_keep_prob, learning_rate):\n",
    "        feed_dict = {self.token_ph: tok_batch,\n",
    "                     self.y_ph: tag_batch,\n",
    "                     self.mask_ph: mask_batch,\n",
    "                     self.dropout_keep_ph: dropout_keep_prob,\n",
    "                     self.learning_rate_ph: learning_rate}\n",
    "        self.sess.run(self.train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个NerNetwork实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nernet = NerNetwork(len(token_vocab),\n",
    "                    len(tag_vocab),\n",
    "                    n_hidden_list=[100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通常希望在每个阶段检查数据集验证部分的得分。在大多数NER任务的情况下，类是不平衡的。而精确度并不是衡量性能的最佳指标。如果我们有95%的O标签，比愚蠢的分类器，总是预测0得到95%的准确率。为了解决这个问题，使用了F1得分。F1得分定义为:\n",
    "![](https://render.githubusercontent.com/render/math?math=F1%20%3D%20%20%5Cfrac%7B2%20P%20R%7D%7BP%20%2B%20R%7D&mode=display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中P为精度，R为召回率。  \n",
    "我们需要写出求值函数。我们需要得到数据集给定部分的所有预测，并计算F1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.ner.evaluation import precision_recall_f1\n",
    "# The function precision_recall_f1 takes two lists: y_true and y_predicted\n",
    "# the tag sequences for each sentences should be merged into one big list \n",
    "from deeppavlov.core.data.utils import zero_pad\n",
    "# zero_pad takes a batch of lists of token indices, pad it with zeros to the\n",
    "# maximal length and convert it to numpy matrix\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def eval_valid(network, batch_generator):\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "    for x, y_true in batch_generator:\n",
    "\n",
    "        # Prepare token indices from tokens batch\n",
    "        x_inds = token_vocab(x) # YOUR CODE HERE\n",
    "\n",
    "        # Pad the indices batch with zeros\n",
    "        x_batch = zero_pad(x_inds) # YOUR CODE HERE\n",
    "\n",
    "        # Get the mask using get_mask\n",
    "        mask = get_mask(x) # YOUR CODE HERE\n",
    "        \n",
    "        # We call the instance of the NerNetwork because we have defined __call__ method\n",
    "        y_inds = network(x_batch, mask)\n",
    "\n",
    "        # For every sentence in the batch extract all tags up to paddings\n",
    "        y_inds = [y_inds[n][:len(x[n])] for n, y in enumerate(y_inds)] # YOUR CODE HERE\n",
    "        y_pred = tag_vocab(y_inds)\n",
    "\n",
    "        # Add fresh predictions \n",
    "        total_true.extend(chain(*y_true))\n",
    "        total_pred.extend(chain(*y_pred))\n",
    "    res = precision_recall_f1(total_true, total_pred, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置参数可以从以下推荐值开始:\n",
    "* batch_size: 32;\n",
    "* n_epochs: 10;\n",
    "* starting value of learning_rate: 0.001\n",
    "* learning_rate_decay: a square root of 2;\n",
    "* dropout_keep_probability equal to 0.7 for training (typical values for dropout probability are ranging from 0.3 to 0.9).  \n",
    "\n",
    "收敛后降低学习率是一种非常有效的学习率管理方法。通常使用2、3和10来降低学习速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # YOUR HYPERPARAMETER HERE\n",
    "n_epochs = 20 # YOUR HYPERPARAMETER HERE\n",
    "learning_rate = 0.001 # YOUR HYPERPARAMETER HERE\n",
    "dropout_keep_prob = 0.5 # YOUR HYPERPARAMETER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们逐批迭代数据集，并将数据传递给训练器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:28:53.519 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 4832 phrases; correct: 3116.\n",
      "\n",
      "precision:  64.49%; recall:  52.44%; FB1:  57.84\n",
      "\n",
      "\tLOC: precision:  69.90%; recall:  69.41%; F1:  69.65 1824\n",
      "\n",
      "\tMISC: precision:  53.96%; recall:  19.96%; F1:  29.14 341\n",
      "\n",
      "\tORG: precision:  51.90%; recall:  44.89%; F1:  48.14 1160\n",
      "\n",
      "\tPER: precision:  70.01%; recall:  57.27%; F1:  63.00 1507\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:29:12.258 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5301 phrases; correct: 4337.\n",
      "\n",
      "precision:  81.81%; recall:  72.99%; FB1:  77.15\n",
      "\n",
      "\tLOC: precision:  86.92%; recall:  82.47%; F1:  84.64 1743\n",
      "\n",
      "\tMISC: precision:  80.29%; recall:  71.15%; F1:  75.45 817\n",
      "\n",
      "\tORG: precision:  75.27%; recall:  62.86%; F1:  68.51 1120\n",
      "\n",
      "\tPER: precision:  81.62%; recall:  71.82%; F1:  76.41 1621\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:29:30.967 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5411 phrases; correct: 4655.\n",
      "\n",
      "precision:  86.03%; recall:  78.34%; FB1:  82.00\n",
      "\n",
      "\tLOC: precision:  91.30%; recall:  84.59%; F1:  87.82 1702\n",
      "\n",
      "\tMISC: precision:  86.21%; recall:  75.27%; F1:  80.37 805\n",
      "\n",
      "\tORG: precision:  79.01%; recall:  72.41%; F1:  75.56 1229\n",
      "\n",
      "\tPER: precision:  85.73%; recall:  77.96%; F1:  81.66 1675\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:29:49.663 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5254 phrases; correct: 4581.\n",
      "\n",
      "precision:  87.19%; recall:  77.10%; FB1:  81.83\n",
      "\n",
      "\tLOC: precision:  93.37%; recall:  85.08%; F1:  89.03 1674\n",
      "\n",
      "\tMISC: precision:  85.80%; recall:  77.33%; F1:  81.35 831\n",
      "\n",
      "\tORG: precision:  81.26%; recall:  73.38%; F1:  77.12 1211\n",
      "\n",
      "\tPER: precision:  85.89%; recall:  71.72%; F1:  78.17 1538\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:30:09.538 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5376 phrases; correct: 4700.\n",
      "\n",
      "precision:  87.43%; recall:  79.10%; FB1:  83.05\n",
      "\n",
      "\tLOC: precision:  93.68%; recall:  87.15%; F1:  90.30 1709\n",
      "\n",
      "\tMISC: precision:  84.97%; recall:  79.07%; F1:  81.91 858\n",
      "\n",
      "\tORG: precision:  81.45%; recall:  74.65%; F1:  77.90 1229\n",
      "\n",
      "\tPER: precision:  86.65%; recall:  74.32%; F1:  80.01 1580\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:30:29.531 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5260 phrases; correct: 4656.\n",
      "\n",
      "precision:  88.52%; recall:  78.36%; FB1:  83.13\n",
      "\n",
      "\tLOC: precision:  94.29%; recall:  85.47%; F1:  89.66 1665\n",
      "\n",
      "\tMISC: precision:  86.88%; recall:  78.31%; F1:  82.37 831\n",
      "\n",
      "\tORG: precision:  83.29%; recall:  74.35%; F1:  78.57 1197\n",
      "\n",
      "\tPER: precision:  87.24%; recall:  74.21%; F1:  80.20 1567\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:30:48.736 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5482 phrases; correct: 4771.\n",
      "\n",
      "precision:  87.03%; recall:  80.29%; FB1:  83.53\n",
      "\n",
      "\tLOC: precision:  91.06%; recall:  88.73%; F1:  89.88 1790\n",
      "\n",
      "\tMISC: precision:  87.41%; recall:  78.31%; F1:  82.61 826\n",
      "\n",
      "\tORG: precision:  79.08%; recall:  77.78%; F1:  78.42 1319\n",
      "\n",
      "\tPER: precision:  88.95%; recall:  74.70%; F1:  81.20 1547\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:31:07.955 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5336 phrases; correct: 4692.\n",
      "\n",
      "precision:  87.93%; recall:  78.96%; FB1:  83.21\n",
      "\n",
      "\tLOC: precision:  93.35%; recall:  86.34%; F1:  89.71 1699\n",
      "\n",
      "\tMISC: precision:  87.41%; recall:  79.83%; F1:  83.45 842\n",
      "\n",
      "\tORG: precision:  82.32%; recall:  75.02%; F1:  78.50 1222\n",
      "\n",
      "\tPER: precision:  86.71%; recall:  74.05%; F1:  79.88 1573\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:31:27.110 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5299 phrases; correct: 4708.\n",
      "\n",
      "precision:  88.85%; recall:  79.23%; FB1:  83.76\n",
      "\n",
      "\tLOC: precision:  94.20%; recall:  86.72%; F1:  90.31 1691\n",
      "\n",
      "\tMISC: precision:  85.61%; recall:  79.39%; F1:  82.39 855\n",
      "\n",
      "\tORG: precision:  83.62%; recall:  76.14%; F1:  79.70 1221\n",
      "\n",
      "\tPER: precision:  88.90%; recall:  73.94%; F1:  80.74 1532\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:31:47.200 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5247 phrases; correct: 4683.\n",
      "\n",
      "precision:  89.25%; recall:  78.81%; FB1:  83.71\n",
      "\n",
      "\tLOC: precision:  93.80%; recall:  87.37%; F1:  90.47 1711\n",
      "\n",
      "\tMISC: precision:  88.65%; recall:  79.61%; F1:  83.89 828\n",
      "\n",
      "\tORG: precision:  85.07%; recall:  76.06%; F1:  80.31 1199\n",
      "\n",
      "\tPER: precision:  87.74%; recall:  71.88%; F1:  79.02 1509\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:32:07.819 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5198 phrases; correct: 4639.\n",
      "\n",
      "precision:  89.25%; recall:  78.07%; FB1:  83.29\n",
      "\n",
      "\tLOC: precision:  93.48%; recall:  86.61%; F1:  89.91 1702\n",
      "\n",
      "\tMISC: precision:  88.37%; recall:  79.93%; F1:  83.94 834\n",
      "\n",
      "\tORG: precision:  83.82%; recall:  75.69%; F1:  79.55 1211\n",
      "\n",
      "\tPER: precision:  89.32%; recall:  70.36%; F1:  78.71 1451\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:32:28.986 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5204 phrases; correct: 4632.\n",
      "\n",
      "precision:  89.01%; recall:  77.95%; FB1:  83.12\n",
      "\n",
      "\tLOC: precision:  93.45%; recall:  86.99%; F1:  90.10 1710\n",
      "\n",
      "\tMISC: precision:  89.12%; recall:  79.07%; F1:  83.79 818\n",
      "\n",
      "\tORG: precision:  83.33%; recall:  76.44%; F1:  79.74 1230\n",
      "\n",
      "\tPER: precision:  88.52%; recall:  69.49%; F1:  77.86 1446\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:32:49.334 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5187 phrases; correct: 4612.\n",
      "\n",
      "precision:  88.91%; recall:  77.62%; FB1:  82.88\n",
      "\n",
      "\tLOC: precision:  93.16%; recall:  86.06%; F1:  89.47 1697\n",
      "\n",
      "\tMISC: precision:  89.33%; recall:  79.93%; F1:  84.37 825\n",
      "\n",
      "\tORG: precision:  84.38%; recall:  76.14%; F1:  80.05 1210\n",
      "\n",
      "\tPER: precision:  87.49%; recall:  69.11%; F1:  77.22 1455\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:33:09.46 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5170 phrases; correct: 4611.\n",
      "\n",
      "precision:  89.19%; recall:  77.60%; FB1:  82.99\n",
      "\n",
      "\tLOC: precision:  93.75%; recall:  86.61%; F1:  90.04 1697\n",
      "\n",
      "\tMISC: precision:  89.08%; recall:  79.61%; F1:  84.08 824\n",
      "\n",
      "\tORG: precision:  83.73%; recall:  76.36%; F1:  79.88 1223\n",
      "\n",
      "\tPER: precision:  88.50%; recall:  68.51%; F1:  77.23 1426\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:33:30.215 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5177 phrases; correct: 4625.\n",
      "\n",
      "precision:  89.34%; recall:  77.84%; FB1:  83.19\n",
      "\n",
      "\tLOC: precision:  93.24%; recall:  86.28%; F1:  89.62 1700\n",
      "\n",
      "\tMISC: precision:  89.24%; recall:  80.04%; F1:  84.39 827\n",
      "\n",
      "\tORG: precision:  84.78%; recall:  75.17%; F1:  79.68 1189\n",
      "\n",
      "\tPER: precision:  88.57%; recall:  70.25%; F1:  78.35 1461\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:33:50.712 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5219 phrases; correct: 4633.\n",
      "\n",
      "precision:  88.77%; recall:  77.97%; FB1:  83.02\n",
      "\n",
      "\tLOC: precision:  92.75%; recall:  87.04%; F1:  89.81 1724\n",
      "\n",
      "\tMISC: precision:  88.10%; recall:  80.26%; F1:  84.00 840\n",
      "\n",
      "\tORG: precision:  85.08%; recall:  74.87%; F1:  79.65 1180\n",
      "\n",
      "\tPER: precision:  87.46%; recall:  70.03%; F1:  77.78 1475\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:34:09.840 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5192 phrases; correct: 4615.\n",
      "\n",
      "precision:  88.89%; recall:  77.67%; FB1:  82.90\n",
      "\n",
      "\tLOC: precision:  92.94%; recall:  86.72%; F1:  89.72 1714\n",
      "\n",
      "\tMISC: precision:  89.96%; recall:  79.72%; F1:  84.53 817\n",
      "\n",
      "\tORG: precision:  83.18%; recall:  74.50%; F1:  78.60 1201\n",
      "\n",
      "\tPER: precision:  88.22%; recall:  69.92%; F1:  78.01 1460\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:34:29.318 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5170 phrases; correct: 4628.\n",
      "\n",
      "precision:  89.52%; recall:  77.89%; FB1:  83.30\n",
      "\n",
      "\tLOC: precision:  94.96%; recall:  85.19%; F1:  89.81 1648\n",
      "\n",
      "\tMISC: precision:  89.78%; recall:  79.07%; F1:  84.08 812\n",
      "\n",
      "\tORG: precision:  83.22%; recall:  76.21%; F1:  79.56 1228\n",
      "\n",
      "\tPER: precision:  88.53%; recall:  71.23%; F1:  78.94 1482\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:34:49.29 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5283 phrases; correct: 4668.\n",
      "\n",
      "precision:  88.36%; recall:  78.56%; FB1:  83.17\n",
      "\n",
      "\tLOC: precision:  93.16%; recall:  86.72%; F1:  89.82 1710\n",
      "\n",
      "\tMISC: precision:  88.88%; recall:  79.72%; F1:  84.05 827\n",
      "\n",
      "\tORG: precision:  81.53%; recall:  75.39%; F1:  78.34 1240\n",
      "\n",
      "\tPER: precision:  88.25%; recall:  72.15%; F1:  79.39 1506\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on valid part of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:35:08.617 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 51363 tokens with 5942 phrases; found: 5180 phrases; correct: 4610.\n",
      "\n",
      "precision:  89.00%; recall:  77.58%; FB1:  82.90\n",
      "\n",
      "\tLOC: precision:  93.26%; recall:  86.55%; F1:  89.78 1705\n",
      "\n",
      "\tMISC: precision:  90.21%; recall:  79.93%; F1:  84.76 817\n",
      "\n",
      "\tORG: precision:  83.36%; recall:  75.09%; F1:  79.01 1208\n",
      "\n",
      "\tPER: precision:  88.00%; recall:  69.27%; F1:  77.52 1450\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for x, y in data_iterator.gen_batches(batch_size, 'train'):\n",
    "        # Convert tokens to indices via Vocab\n",
    "        x_inds = token_vocab(x) # YOUR CODE \n",
    "        # Convert tags to indices via Vocab\n",
    "        y_inds = tag_vocab(y) # YOUR CODE \n",
    "        \n",
    "        # Pad every sample with zeros to the maximal length\n",
    "        x_batch = zero_pad(x_inds)\n",
    "        y_batch = zero_pad(y_inds)\n",
    "\n",
    "        mask = get_mask(x)\n",
    "        nernet.train_on_batch(x_batch, y_batch, mask, dropout_keep_prob, learning_rate)\n",
    "    print('Evaluating the model on valid part of the dataset')\n",
    "    eval_valid(nernet, data_iterator.gen_batches(batch_size, 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估测试数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-20 17:35:10.85 DEBUG in 'deeppavlov.models.ner.evaluation'['evaluation'] at line 213: processed 46436 tokens with 5648 phrases; found: 4533 phrases; correct: 3728.\n",
      "\n",
      "precision:  82.24%; recall:  66.01%; FB1:  73.23\n",
      "\n",
      "\tLOC: precision:  88.05%; recall:  80.88%; F1:  84.31 1532\n",
      "\n",
      "\tMISC: precision:  77.05%; recall:  70.80%; F1:  73.79 645\n",
      "\n",
      "\tORG: precision:  79.23%; recall:  61.11%; F1:  69.00 1281\n",
      "\n",
      "\tPER: precision:  80.65%; recall:  53.62%; F1:  64.41 1075\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_valid(nernet, data_iterator.gen_batches(batch_size, 'test'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用模型预测我们自己说的话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Petr', 'stole', 'my', 'vodka', 'in', 'America']\n",
      "['B-PER', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Petr stole my vodka in America'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "print(x[0])\n",
    "print(tag_vocab(y_inds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wu', 'Jiahang', 'is', 'the', 'fastest', 'man', 'alive', 'in', 'China']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Wu Jiahang is the fastest man alive in China'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "print(x[0])\n",
    "print(tag_vocab(y_inds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YuLinzhu', 'is', 'the', 'stupidest', 'woman', 'alive', 'in', 'China']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'YuLinzhu is the stupidest woman alive in China'\n",
    "x = [sentence.split()]\n",
    "\n",
    "x_inds = token_vocab(x)\n",
    "x_batch = zero_pad(x_inds)\n",
    "mask = get_mask(x)\n",
    "y_inds = nernet(x_batch, mask)\n",
    "print(x[0])\n",
    "print(tag_vocab(y_inds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
