{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  用CNN(卷积神经网络)识别新闻数据中的命名实体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*在这个教程中，我们会用卷积神经网络（convolutional neural network,CNN）去解决命名实体识别（Named Entity Recognition,NER）的问题。*\n",
    "  \n",
    "*命名实体识别是自然语言处理中经常遇到的问题，它的作用是从文本中抽取出一些实体，例如人、机构、地点等等。* \n",
    "  \n",
    "*在这里，我们会做些实验，在CoNLL-2003数据集中的不同新闻中，识别出命名实体。*  \n",
    "  \n",
    "  例如，我们想从下面这句话中解析出人和机构的名字\n",
    ">Yan Goodfellow works for Google Brain\n",
    "  \n",
    "  NER模型需要提供如下的标签(tags)序列:\n",
    ">B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "  \n",
    "  \n",
    "  这里有两个前缀：  \n",
    "*B-*代表着实体的beginning  \n",
    "*I-*代表着实体的inside  \n",
    "*O*代表没有标签  \n",
    "带有这种前缀的标记称为BIO标记(BIO markup),引入此标记是为了区分具有相似类型的后续实体。  \n",
    "  \n",
    "  \n",
    "  解决这种问题需要用到神经网络的相关知识，尤其是**卷积神经网络**。  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "下面的单元格将把这个任务所需的所有数据下载到文件夹 \\data 中，库中的下载工具用来下载和提取文件的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 16:51:27.649 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "2018-08-19 16:51:28.650 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 302 None\n",
      "2018-08-19 16:51:28.659 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): 202.112.144.234:80\n",
      "2018-08-19 16:51:28.671 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://202.112.144.234:80 \"GET /files/3146000001CF3D15/lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz HTTP/1.1\" 200 957092\n",
      "2018-08-19 16:51:28.675 INFO in 'deeppavlov.core.data.utils'['utils'] at line 62: Downloading from http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz to data/conll2003_v2.tar.gz\n",
      "100%|██████████| 957k/957k [00:00<00:00, 1.30MB/s]\n",
      "2018-08-19 16:51:29.419 INFO in 'deeppavlov.core.data.utils'['utils'] at line 200: Extracting data/conll2003_v2.tar.gz archive into data\n"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载CoNLL-2003命名实体识别语料库\n",
    "这里我们将运用到一个包含带有命名实体标签的推特文章的语料库(corpus)。一个典型的命名实体识别数据文件包含符号（*tokens*）（词或标点符号）和标签（*tags*），它们被空格分隔开。有的时候一些附加信息，比如 POS-tags 也是包含在其中的。\n",
    "不同的文件是用 **-DOCSTART-** 开头的一行分隔开的，不同的句子是用一行空白行分隔开的。\n",
    "例如：\n",
    "~~~\n",
    "  \n",
    "  -DOCSTART- -X- -X- O\n",
    "\n",
    "  EU NNP B-NP B-ORG  \n",
    "  rejects VBZ B-VP O  \n",
    "  German JJ B-NP B-MISC  \n",
    "  call NN I-NP O  \n",
    "  to TO B-VP O  \n",
    "  boycott VB I-VP O  \n",
    "  British JJ B-NP B-MISC  \n",
    "  lamb NN I-NP O  \n",
    "  . . O O  \n",
    "\n",
    "  Peter NNP B-NP B-PER  \n",
    "  Blackburn NNP I-NP I-PER  \n",
    "  \n",
    "~~~\n",
    "这个教程中我们只关注tokens和tags（也就是每行的第一个元素和最后一个元素）,而忽略掉两者之间的POS元素。  \n",
    "  \n",
    "\n",
    "我们先新建一个Conll2003DatasetReader类，用来读取数据集。它返回的是一个dictionary包含train,test,valid这三个field，每个field存储着一些sample构成的list，每个sample是由tokens和tags构成的tuple,其中tokens和tags是list。\n",
    "下面的例子描述了这个dictionary的结构，它由NerDatasetReader类中的read()方法返回：  \n",
    "~~~\n",
    "{'train': [(['Mr.', 'Dwag', 'are', 'derping', 'around'], ['B-PER', 'I-PER', 'O', 'O', 'O']), ....],\n",
    " 'valid': [...],\n",
    " 'test': [...]}\n",
    "\n",
    "~~~  \n",
    "数据集分为三个部分：  \n",
    "1.train: 用来训练模型  \n",
    "2.valid: 用来评估以及参数调优  \n",
    "3.test:用来最终评估模型  \n",
    "这三个部分分别存在三个txt文件中。  \n",
    "  \n",
    "我们会用库中的Conll2003DatasetReader类来读取数据，也就是把文本转换成如上所说的形式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应该始终了解我们处理的数据类型，因此，我们用下面的代码把它们打印出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOCSTART>\tO\n",
      "\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n",
      "The\tO\n",
      "European\tB-ORG\n",
      "Commission\tI-ORG\n",
      "said\tO\n",
      "on\tO\n",
      "Thursday\tO\n",
      "it\tO\n",
      "disagreed\tO\n",
      "with\tO\n",
      "German\tB-MISC\n",
      "advice\tO\n",
      "to\tO\n",
      "consumers\tO\n",
      "to\tO\n",
      "shun\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      "until\tO\n",
      "scientists\tO\n",
      "determine\tO\n",
      "whether\tO\n",
      "mad\tO\n",
      "cow\tO\n",
      "disease\tO\n",
      "can\tO\n",
      "be\tO\n",
      "transmitted\tO\n",
      "to\tO\n",
      "sheep\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset['train'][:5]:\n",
    "    for token,tag in zip(*sample):\n",
    "        print('%s\\t%s' % (token,tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备字典\n",
    "为了训练出来一个神经网络，我们需要用到两个映射（mapping）：  \n",
    "* {token}$\\to${token id}: 为当前的token处理嵌入矩阵中的行\n",
    "* {tag}$\\to${tag id}:制造one-hot地面真值概率分布向量，用于计算网络输出损耗。  \n",
    "  \n",
    "库中的 SimpleVocabulary 将会执行这些映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将会为token和tag准备字典。有时词汇表中有一些特殊的token，例如一个未知的单词标记，每当我们遇到词汇表之外的单词时就会使用它。这种情况下，我们就会用< UNK > 这种特殊的记号来表示词汇表之外的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 18:18:25.903 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n",
      "2018-08-19 18:18:25.904 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 53: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<UNK>']\n",
    "token_vocab = SimpleVocabulary(special_tokens, save_path='model/token.dict')\n",
    "tag_vocab = SimpleVocabulary(save_path='model/tag.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们在数据的训练部分中加入词汇表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_by_sentences = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentences = [tags for tokens, tags in dataset['train']]# 这是list\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentences)\n",
    "tag_vocab.fit(all_tags_by_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试得到索引，请记住，我们正在使用以下结构的批次:\n",
    "~~~\n",
    "[['utt0_tok0', 'utt1_tok1', ...], ['utt1_tok0', 'utt1_tok1', ...], ...]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10167, 6, 168, 7, 6097, 5518, 1865]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([['How', 'to', 'do', 'a', 'barrel', 'roll', '?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [3, 5]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['O', 'O', 'O'], ['B-ORG', 'I-ORG']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab([['I-ORG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们试试从索引到token的转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['at',\n",
       "  '2',\n",
       "  'Germany',\n",
       "  'said',\n",
       "  'around',\n",
       "  'news',\n",
       "  'goals',\n",
       "  'run',\n",
       "  'party',\n",
       "  '26']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "token_vocab([np.random.randint(0, 512, size=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', ',']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vocab([[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集迭代器(Iterator)\n",
    "神经网络通常是分批训练的。这意味着网络的权值更新是基于每一次的多个序列。每一批中的所有序列需要具有相同的长度。因此，我们将向它们填充一个特殊的< UKN >记号。同样，token和tag也必须填充它。为循环神经网络(Recurrent Neural Network，RNN)提供序列长度是很好的实践，所以它可以跳过对填充部件的计算。我们在这里供批处理函数batches_generator以节省时间。  \n",
    "  \n",
    "  批量生成的一个重要概念是打乱(shuffling)。打乱是从数据集中随机抽取样本,对打乱后的数据进行训练是很重要的，因为从同一类抽取的大量结果样本可能导致模型太过于”纯净“。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从加载的数据集中创建数据集迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = DataLearningIterator(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['BUDAPEST', '1996-08-23'],\n",
       "  ['-',\n",
       "   'Aleix',\n",
       "   'Vidal-Quadras',\n",
       "   '-',\n",
       "   'Catalan',\n",
       "   'nationalists',\n",
       "   'are',\n",
       "   'demanding',\n",
       "   'my',\n",
       "   'defenestration']),\n",
       " (['B-LOC', 'O'],\n",
       "  ['O', 'B-PER', 'I-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O']))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(2, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['PUT', 'D', '94.00', 'PCT', '0.94', 'DEM', '2.40', 'PCT', '101.40', 'X'],\n",
       "  ['SOCCER',\n",
       "   '-',\n",
       "   'ETHIOPIA',\n",
       "   'BEAT',\n",
       "   'UGANDA',\n",
       "   'ON',\n",
       "   'PENALTIES',\n",
       "   'IN',\n",
       "   'AFRICAN',\n",
       "   'NATIONS',\n",
       "   'CUP',\n",
       "   '.'],\n",
       "  ['Croatian',\n",
       "   'lending',\n",
       "   'rate',\n",
       "   'falls',\n",
       "   'to',\n",
       "   '8.0',\n",
       "   'vs',\n",
       "   '9.1',\n",
       "   'pct',\n",
       "   '.'],\n",
       "  ['NOTE', '-', 'Orii', 'Corp', 'makes', 'automation', 'equipment', '.'],\n",
       "  ['\"',\n",
       "   'Therefore',\n",
       "   'I',\n",
       "   'have',\n",
       "   'some',\n",
       "   'doubts',\n",
       "   'about',\n",
       "   'achieving',\n",
       "   'some',\n",
       "   'form',\n",
       "   'of',\n",
       "   'reconciliation',\n",
       "   '.']),\n",
       " (['O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'I-MISC',\n",
       "   'I-MISC',\n",
       "   'O'],\n",
       "  ['B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iterator.gen_batches(5, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成掩码\n",
    "关于生成训练数据的最后一件事。我们需要生成一个二进制掩码，在这个掩码中，token代表1，其他代表是0。  \n",
    "这个掩码将阻止通过填充来反向传播。  \n",
    "此类掩码的一个实例:\n",
    "~~~\n",
    "[[1, 1, 0, 0, 0],\n",
    " [1, 1, 1, 1, 1]]\n",
    "~~~\n",
    "代表这些句子：\n",
    "~~~\n",
    " [['The', 'roof'],\n",
    "  ['This', 'is', 'my', 'domain', '!']]\n",
    "~~~\n",
    "掩码长度必须等于批次中句子的最大长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.preprocessors.mask import Mask\n",
    "get_mask = Mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立一个循环神经网络(RNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
