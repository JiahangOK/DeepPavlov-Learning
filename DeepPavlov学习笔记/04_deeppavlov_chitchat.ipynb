{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPavlov的 序列-序列模型 教程\n",
    "在本教程中，我们将在DeepPavlov中实现序列-序列模型。 （论文链接：https://arxiv.org/abs/1409.3215 ） \n",
    "  \n",
    "序列-序列指将输入序列映射到目标序列，序列-序列模型由编码器和解码器两大部分组成。编码器用于将输入序列编码为密集式表示，解码器使用这种密集式表示生成目标序列。  \n",
    "  \n",
    "![](https://github.com/deepmipt/DeepPavlov/raw/c7896c6db96f43f57cacd9a6a471e37cb70bf07a/examples/tutorials/img/seq2seq.png)  \n",
    "  \n",
    "上面这个图片中,输入序列是ABC，使用特殊token < EOS >(序列末尾)作为指示符，来指示开始解码目标序列WXYZ。  \n",
    "  \n",
    "为了在DeepPavlov中实现这个模型，我们需要编写一些DeepPavlov的抽象代码:  \n",
    "- DatasetReader：读取数据\n",
    "- DatasetIterator：生成批次\n",
    "- Vocabulary：将单词转化成索引\n",
    "- Model：训练并使用模型\n",
    "- 以及其它一些用于预处理和后处理的组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import deeppavlov\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载和解压数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 16:26:27.237 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "2018-08-23 16:26:27.907 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://files.deeppavlov.ai:80 \"GET /datasets/personachat_v2.tar.gz HTTP/1.1\" 200 223217972\n",
      "2018-08-23 16:26:27.915 INFO in 'deeppavlov.core.data.utils'['utils'] at line 65: Downloading from http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz to /home/jiahang/Jiahang_Jupyter_Note/personachat/personachat_v2.tar.gz\n",
      "100%|██████████| 223M/223M [03:10<00:00, 1.17MB/s]   \n",
      "2018-08-23 16:29:38.356 INFO in 'deeppavlov.core.data.utils'['utils'] at line 149: Extracting personachat/personachat_v2.tar.gz archive into personachat\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz', './personachat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetReader\n",
    "DatasetReader用于从文件中读取和解析数据。  \n",
    "  \n",
    "这里，我们定义一个新的类PersonaChatDatasetReader读取 PersonaChat数据集。  \n",
    "  \n",
    "PersonaChat数据集由对话框和用户的个性组成。  \n",
    "  \n",
    "用户个性用四个句子来描述，例如:  \n",
    "~~~\n",
    "i like to remodel homes.\n",
    "i like to go hunting.\n",
    "i like to shoot a bow.\n",
    "my favorite holiday is halloween.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import build_model_from_config\n",
    "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "from deeppavlov.core.common.registry import register\n",
    "\n",
    "@register('personachat_dataset_reader')\n",
    "class PersonaChatDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    PersonaChat dataset from\n",
    "    Zhang S. et al. Personalizing Dialogue Agents: I have a dog, do you have pets too?\n",
    "    https://arxiv.org/abs/1801.07243\n",
    "    Also, this dataset is used in ConvAI2 http://convai.io/\n",
    "    This class reads dataset to the following format:\n",
    "    [{\n",
    "        'persona': [list of persona sentences],\n",
    "        'x': input utterance,\n",
    "        'y': output utterance,\n",
    "        'dialog_history': list of previous utterances\n",
    "        'candidates': [list of candidate utterances]\n",
    "        'y_idx': index of y utt in candidates list\n",
    "      },\n",
    "       ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    def read(self, dir_path: str, mode='self_original'):\n",
    "        dir_path = Path(dir_path)\n",
    "        dataset = {}\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            dataset[dt] = self._parse_data(dir_path / '{}_{}.txt'.format(dt, mode))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_data(filename):\n",
    "        examples = []\n",
    "        print(filename)\n",
    "        curr_persona = []\n",
    "        curr_dialog_history = []\n",
    "        persona_done = False\n",
    "        with filename.open('r') as fin:\n",
    "            for line in fin:\n",
    "                line = ' '.join(line.strip().split(' ')[1:])\n",
    "                your_persona_pref = 'your persona: '\n",
    "                if line[:len(your_persona_pref)] == your_persona_pref and persona_done:\n",
    "                    curr_persona = [line[len(your_persona_pref):]]\n",
    "                    curr_dialog_history = []\n",
    "                    persona_done = False\n",
    "                elif line[:len(your_persona_pref)] == your_persona_pref:\n",
    "                    curr_persona.append(line[len(your_persona_pref):])\n",
    "                else:\n",
    "                    persona_done = True\n",
    "                    x, y, _, candidates = line.split('\\t')\n",
    "                    candidates = candidates.split('|')\n",
    "                    example = {\n",
    "                        'persona': curr_persona,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'dialog_history': curr_dialog_history[:],\n",
    "                        'candidates': candidates,\n",
    "                        'y_idx': candidates.index(y)\n",
    "                    }\n",
    "                    curr_dialog_history.extend([x, y])\n",
    "                    examples.append(example)\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personachat/train_self_original.txt\n",
      "personachat/valid_self_original.txt\n",
      "personachat/test_self_original.txt\n"
     ]
    }
   ],
   "source": [
    "data = PersonaChatDatasetReader().read('./personachat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看数据集的大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 65719\n",
      "valid 7801\n",
      "test 7512\n"
     ]
    }
   ],
   "source": [
    "for k in data:\n",
    "    print(k, len(data[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['i like to remodel homes.',\n",
       "  'i like to go hunting.',\n",
       "  'i like to shoot a bow.',\n",
       "  'my favorite holiday is halloween.'],\n",
       " 'x': 'hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .',\n",
       " 'y': 'you must be very fast . hunting is one of my favorite hobbies .',\n",
       " 'dialog_history': [],\n",
       " 'candidates': ['my mom was single with 3 boys , so we never left the projects .',\n",
       "  'i try to wear all black every day . it makes me feel comfortable .',\n",
       "  'well nursing stresses you out so i wish luck with sister',\n",
       "  'yeah just want to pick up nba nfl getting old',\n",
       "  'i really like celine dion . what about you ?',\n",
       "  'no . i live near farms .',\n",
       "  'i wish i had a daughter , i am a boy mom . they are beautiful boys though still lucky',\n",
       "  'yeah when i get bored i play gone with the wind my favorite movie .',\n",
       "  'hi how are you ? i am eating dinner with my hubby and 2 kids .',\n",
       "  'were you married to your high school sweetheart ? i was .',\n",
       "  'that is great to hear ! are you a competitive rider ?',\n",
       "  'hi , i am doing ok . i am a banker . how about you ?',\n",
       "  'i am 5 years old',\n",
       "  'hi there . how are you today ?',\n",
       "  'i totally understand how stressful that can be .',\n",
       "  'yeah sometimes you do not know what you are actually watching',\n",
       "  'mother taught me to cook ! we are looking for an exterminator .',\n",
       "  'i enjoy romantic movie . what is your favorite season ? mine is summer .',\n",
       "  'editing photos takes a lot of work .',\n",
       "  'you must be very fast . hunting is one of my favorite hobbies .'],\n",
       " 'y_idx': 19}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iterator\n",
    "数据集迭代器（Dataset iterator） 用于从已解析的数据集(DatasetReader)生成批次。  \n",
    "  \n",
    "让我们只从已解析的数据集中提取x和y，并用它们从句子x来预测句子y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n",
    "\n",
    "@register('personachat_iterator')\n",
    "class PersonaChatIterator(DataLearningIterator):\n",
    "    def split(self, *args, **kwargs):\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            setattr(self, dt, self._to_tuple(getattr(self, dt)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tuple(data):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of (x, y)\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: (x['x'], x['y']), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看分好批次中的数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: i think i will look into it .\n",
      "y: that would be good to do . you should try to look it up on youtube\n",
      "----------\n",
      "x: i live in new orleans .\n",
      "y: what do you do ? i have a toothpick business ,\n",
      "----------\n",
      "x: i am blue , because i was born male , then transitioned to female 3 years ago .\n",
      "y: that is good that you transitioned why would you feel blue ?\n",
      "----------\n",
      "x: after school hard to find a job in the city your town sounds fun\n",
      "y: it is nice . i can enjoy the lake and a few books every weekend .\n",
      "----------\n",
      "x: very exact of you ! the hair was reddish so i think it was my own . lol\n",
      "y: oh my favorite color , red ! i would love your hair\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "iterator = PersonaChatIterator(data)\n",
    "batch = [el for el in iterator.gen_batches(5, 'train')][0]\n",
    "for x, y in zip(*batch):\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "分词器（Tokenizer）用于从话语中提取token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/jiahang/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/jiahang/env/nltk_data'\n    - '/home/jiahang/env/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0d254026f7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazyTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hello my friend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DeepPavlov/deeppavlov/models/preprocessors/lazy_tokenizer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepPavlov/deeppavlov/models/preprocessors/lazy_tokenizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/jiahang/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/jiahang/env/nltk_data'\n    - '/home/jiahang/env/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.lazy_tokenizer import LazyTokenizer\n",
    "tokenizer = LazyTokenizer()\n",
    "tokenizer(['Hello my friend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "词汇表（Vocabulary）准备从token到token索引的映射，它使用‘train’中的数据构建这个映射。  \n",
    "  \n",
    "我们将实现DialogVocab类(继承SimpleVocabulary类)，它将x和y话语中的所有token都添加到词汇表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "@register('dialog_vocab')\n",
    "class DialogVocab(SimpleVocabulary):\n",
    "    def fit(self, *args):\n",
    "        tokens = chain(*args)\n",
    "        super().fit(tokens)\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        indices_batch = []\n",
    "        for utt in batch:\n",
    "            tokens = [self[token] for token in utt]\n",
    "            indices_batch.append(tokens)\n",
    "        return indices_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来创建一个DialogVocab的实例，定义保存和加载的路径，加入词汇表和特殊标记集的token的最小频率。  \n",
    "  \n",
    "特殊标记是：\n",
    "- < PAD > - 填充\n",
    "- < BOS > - 序列的起点\n",
    "- < EOS > - 序列的终点\n",
    "- < UNK > - 未知标记（没有出现在词汇表中的token）  \n",
    "  \n",
    "并把他们配置在从x和y中获取的token中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = DialogVocab(\n",
    "    save_path='./vocab.dict',\n",
    "    load_path='./vocab.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>','<BOS>', '<EOS>', '<UNK>',),\n",
    "    unk_token='<UNK>'\n",
    ")\n",
    "\n",
    "vocab.fit(tokenizer(iterator.get_instances(data_type='train')[0]), tokenizer(iterator.get_instances(data_type='train')[1]))\n",
    "vocab.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train数据集中最常见的10个token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词汇表中的token数量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们使用构建好的的词汇表来编码一些分好词的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([['<BOS>', 'hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this', '<EOS>', '<PAD>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "为了将token索引序列传递给神经模型，我们应该使它们的长度相等。  \n",
    "  \n",
    "如果序列太短，我们在序列的末尾添加< PAD > 符号。  \n",
    "如果序列太长，我们就把它剪掉。  \n",
    "  \n",
    "SentencePadder类实现这个功能，它也可以加 < BOS > 和 < EOS > 标记 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.component import Component\n",
    "\n",
    "@register('sentence_padder')\n",
    "class SentencePadder(Component):\n",
    "    def __init__(self, length_limit, pad_token_id=0, start_token_id=1, end_token_id=2, *args, **kwargs):\n",
    "        self.length_limit = length_limit\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i][:self.length_limit]\n",
    "            batch[i] = [self.start_token_id] + batch[i] + [self.end_token_id]\n",
    "            batch[i] += [self.pad_token_id] * (self.length_limit + 2 - len(batch[i]))\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = SentencePadder(length_limit=6)\n",
    "vocab(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq 模型\n",
    "这个模型由两个主要的组件组成：编码器（encoder）和解码器（decoder）。  \n",
    "  \n",
    "我们可以独立实现它们，然后将它们放在一个Seq2Seq模型中。\n",
    "  \n",
    "### 编码器（Encoder）\n",
    "编码器构建输入序列的隐藏表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs, inputs_len, embedding_matrix, cell_size, keep_prob=1.0):\n",
    "    # inputs: tf.int32 tensor with shape bs x seq_len with token ids\n",
    "    # inputs_len: tf.int32 tensor with shape bs\n",
    "    # embedding_matrix: tf.float32 tensor with shape vocab_size x vocab_dim\n",
    "    # cell_size: hidden size of recurrent cell\n",
    "    # keep_prob: dropout keep probability\n",
    "    with tf.variable_scope('encoder'):\n",
    "        # first of all we should embed every token in input sequence (use tf.nn.embedding_lookup, don't forget about dropout)\n",
    "        x_emb = tf.nn.dropout(tf.nn.embedding_lookup(embedding_matrix, inputs), keep_prob=keep_prob)\n",
    "        \n",
    "        # define recurrent cell (LSTM or GRU)\n",
    "        encoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='encoder_cell')\n",
    "        \n",
    "        # use tf.nn.dynamic_rnn to encode input sequence, use actual length of input sequence\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell, inputs=x_emb, sequence_length=inputs_len, dtype=tf.float32)\n",
    "    return encoder_outputs, encoder_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查编码器实现:  \n",
    "下一个单元格输出形状是 32 x 10 x 100 和 32 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # bs x seq_len\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # bs x seq_len\n",
    "inputs_len = tf.reduce_sum(mask, axis=1)\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim])\n",
    "\n",
    "encoder(inputs, inputs_len, embedding_matrix, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器（Decoder）\n",
    "解码器使用编码器的输出和编码器状态产生输出序列。  \n",
    "  \n",
    "这里，你要做的是：\n",
    "- 定义你的解码块 decoder_cell (GRU或LSTM)\n",
    "  \n",
    "它将成为您的基础seq2seq模型。  \n",
    "  \n",
    "并且，为了改进模型:\n",
    "- 添加Teacher Forcing（教师强制算法，在MT && Abstractive Summarization的encoder训练中比较常用）\n",
    "- 添加Attention Mechanism（注意力机制）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encoder_outputs, encoder_state, embedding_matrix, mask,\n",
    "            cell_size, max_length, y_ph,\n",
    "            start_token_id=1, keep_prob=1.0,\n",
    "            teacher_forcing_rate_ph=None,\n",
    "            use_attention=False, is_train=True):\n",
    "    # decoder\n",
    "    # encoder_outputs: tf.float32 tensor with shape bs x seq_len x encoder_cell_size\n",
    "    # encoder_state: tf.float32 tensor with shape bs x encoder_cell_size\n",
    "    # embedding_matrix: tf.float32 tensor with shape vocab_size x vocab_dim\n",
    "    # mask: tf.int32 tensor with shape bs x seq_len with zeros for masked sequence elements\n",
    "    # cell_size: hidden size of recurrent cell\n",
    "    # max_length: max length of output sequence\n",
    "    # start_token_id: id of <BOS> token in vocabulary\n",
    "    # keep_prob: dropout keep probability\n",
    "    # teacher_forcing_rate_ph: rate of using teacher forcing on each decoding step\n",
    "    # use_attention: use attention on encoder outputs or use only encoder_state\n",
    "    # is_train: is it training or inference? at inference time we can't use teacher forcing\n",
    "    with tf.variable_scope('decoder'):\n",
    "        # define decoder recurrent cell\n",
    "        decoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='decoder_cell')\n",
    "        \n",
    "        # initial value of output_token on previsous step is start_token\n",
    "        output_token = tf.ones(shape=(tf.shape(encoder_outputs)[0],), dtype=tf.int32) * start_token_id\n",
    "        # let's define initial value of decoder state with encoder_state\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        pred_tokens = []\n",
    "        logits = []\n",
    "\n",
    "        # use for loop to sequentially call recurrent cell\n",
    "        for i in range(max_length):\n",
    "            \"\"\"\n",
    "            TEACHER FORCING\n",
    "            # here you can try to implement teacher forcing for your model\n",
    "            # details about teacher forcing are explained further in tutorial\n",
    "            \n",
    "            # pseudo code:\n",
    "            NOTE THAT FOLLOWING CONDITIONS SHOULD BE EVALUATED AT GRAPH RUNTIME\n",
    "            use tf.cond and tf.logical operations instead of python if\n",
    "            \n",
    "            if i > 0 and is_train and random_value < teacher_forcing_rate_ph:\n",
    "                input_token = y_ph[:, i-1]\n",
    "            else:\n",
    "                input_token = output_token\n",
    "\n",
    "            input_token_emb = tf.nn.embedding_lookup(embedding_matrix, input_token)\n",
    "            \n",
    "            \"\"\"\n",
    "            if i > 0:\n",
    "                input_token_emb = tf.cond(\n",
    "                                      tf.logical_and(\n",
    "                                          is_train,\n",
    "                                          tf.random_uniform(shape=(), maxval=1) <= teacher_forcing_rate_ph\n",
    "                                      ),\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, y_ph[:, i-1]), # teacher forcing\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, output_token)\n",
    "                                      )\n",
    "            else:\n",
    "                input_token_emb = tf.nn.embedding_lookup(embedding_matrix, output_token)\n",
    "\n",
    "            \"\"\"\n",
    "            ATTENTION MECHANISM\n",
    "            # here you can add attention to your model\n",
    "            # you can find details about attention further in tutorial\n",
    "            \"\"\"            \n",
    "            if use_attention:\n",
    "                # compute attention and concat attention vector to input_token_emb\n",
    "                att = dot_attention(encoder_outputs, decoder_state, mask, scope='att')\n",
    "                input_token_emb = tf.concat([input_token_emb, att], axis=-1)\n",
    "\n",
    "\n",
    "            input_token_emb = tf.nn.dropout(input_token_emb, keep_prob=keep_prob)\n",
    "            # call recurrent cell\n",
    "            decoder_outputs, decoder_state = decoder_cell(input_token_emb, decoder_state)\n",
    "            decoder_outputs = tf.nn.dropout(decoder_outputs, keep_prob=keep_prob)\n",
    "            # project decoder output to embeddings dimension\n",
    "            embeddings_dim = embedding_matrix.get_shape()[1]\n",
    "            output_proj = tf.layers.dense(decoder_outputs, embeddings_dim, activation=tf.nn.tanh,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name='proj', reuse=tf.AUTO_REUSE)\n",
    "            # compute logits\n",
    "            output_logits = tf.matmul(output_proj, embedding_matrix, transpose_b=True)\n",
    "\n",
    "            logits.append(output_logits)\n",
    "            output_probs = tf.nn.softmax(output_logits)\n",
    "            output_token = tf.argmax(output_probs, axis=-1)\n",
    "            pred_tokens.append(output_token)\n",
    "\n",
    "        y_pred_tokens = tf.transpose(tf.stack(pred_tokens, axis=0), [1, 0])\n",
    "        y_logits = tf.transpose(tf.stack(logits, axis=0), [1, 0, 2])\n",
    "    return y_pred_tokens, y_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个单元的输出应该是形状:\n",
    "~~~\n",
    "32 x 10\n",
    "32 x 10 x 100\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # bs x seq_len\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # bs x seq_len\n",
    "inputs_len = tf.reduce_sum(mask, axis=1)\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim])\n",
    "\n",
    "teacher_forcing_rate = tf.random_uniform(shape=())\n",
    "y = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32)\n",
    "\n",
    "encoder_outputs, encoder_state = encoder(inputs, inputs_len, embedding_matrix, hidden_dim)\n",
    "decoder(encoder_outputs, encoder_state, embedding_matrix, mask, hidden_dim, max_length=10,\n",
    "        y_ph=y, teacher_forcing_rate_ph=teacher_forcing_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型\n",
    "Seq2Seq 模型继承TFModel类，并且实现如下方法：\n",
    "- train_on_batch - 在训练阶段调用这个方法\n",
    "- \\_\\_call\\_\\_ - 调用这个方法用来进行预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.tf_model import TFModel\n",
    "\n",
    "@register('seq2seq')\n",
    "class Seq2Seq(TFModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        # hyperparameters\n",
    "        \n",
    "        # dimension of word embeddings\n",
    "        self.embeddings_dim = kwargs.get('embeddings_dim', 100)\n",
    "        # size of recurrent cell in encoder and decoder\n",
    "        self.cell_size = kwargs.get('cell_size', 200)\n",
    "        # dropout keep_probability\n",
    "        self.keep_prob = kwargs.get('keep_prob', 0.8)\n",
    "        # learning rate\n",
    "        self.learning_rate = kwargs.get('learning_rate', 3e-04)\n",
    "        # max length of output sequence\n",
    "        self.max_length = kwargs.get('max_length', 20)\n",
    "        self.grad_clip = kwargs.get('grad_clip', 5.0)\n",
    "        self.start_token_id = kwargs.get('start_token_id', 1)\n",
    "        self.vocab_size = kwargs.get('vocab_size', 11595)\n",
    "        self.teacher_forcing_rate = kwargs.get('teacher_forcing_rate', 0.0)\n",
    "        self.use_attention = kwargs.get('use_attention', False)\n",
    "        \n",
    "        # create tensorflow session to run computational graph in it\n",
    "        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        self.sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=self.sess_config)\n",
    "        \n",
    "        self.init_graph()\n",
    "        \n",
    "        # define train op\n",
    "        self.train_op = self.get_train_op(self.loss, self.lr_ph,\n",
    "                                          optimizer=tf.train.AdamOptimizer,\n",
    "                                          clip_norm=self.grad_clip)\n",
    "        # initialize graph variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        # load saved model if there is one\n",
    "        if self.load_path is not None:\n",
    "            self.load()\n",
    "        \n",
    "    def init_graph(self):\n",
    "        # create placeholders\n",
    "        self.init_placeholders()\n",
    "\n",
    "        self.x_mask = tf.cast(self.x_ph, tf.int32) \n",
    "        self.y_mask = tf.cast(self.y_ph, tf.int32) \n",
    "        \n",
    "        self.x_len = tf.reduce_sum(self.x_mask, axis=1)\n",
    "        \n",
    "        # create embeddings matrix for tokens\n",
    "        self.embeddings = tf.Variable(tf.random_uniform((self.vocab_size, self.embeddings_dim), -0.1, 0.1, name='embeddings'), dtype=tf.float32)\n",
    "\n",
    "        # encoder\n",
    "        encoder_outputs, encoder_state = encoder(self.x_ph, self.x_len, self.embeddings, self.cell_size, self.keep_prob_ph)\n",
    "\n",
    "        # decoder\n",
    "        self.y_pred_tokens, y_logits = decoder(encoder_outputs, encoder_state, self.embeddings, self.x_mask,\n",
    "                                                      self.cell_size, self.max_length,\n",
    "                                                      self.y_ph, self.start_token_id, self.keep_prob_ph,\n",
    "                                                      self.teacher_forcing_rate_ph, self.use_attention, self.is_train_ph)\n",
    "        \n",
    "        # loss\n",
    "        self.y_ohe = tf.one_hot(self.y_ph, depth=self.vocab_size)\n",
    "        self.y_mask = tf.cast(self.y_mask, tf.float32)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y_ohe, logits=y_logits) * self.y_mask\n",
    "        self.loss = tf.reduce_sum(self.loss) / tf.reduce_sum(self.y_mask)\n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        # placeholders for inputs\n",
    "        self.x_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='x_ph')\n",
    "        # at inference time y_ph is used (y_ph exists in computational graph)  when teacher forcing is activated, so we add dummy default value\n",
    "        # this dummy value is not actually used at inference\n",
    "        self.y_ph = tf.placeholder_with_default(tf.zeros_like(self.x_ph), shape=(None,None), name='y_ph')\n",
    "\n",
    "        # placeholders for model parameters\n",
    "        self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[], name='lr_ph')\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n",
    "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n",
    "        self.teacher_forcing_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='teacher_forcing_rate_ph')\n",
    "            \n",
    "    def _build_feed_dict(self, x, y=None):\n",
    "        feed_dict = {\n",
    "            self.x_ph: x,\n",
    "        }\n",
    "        if y is not None:\n",
    "            feed_dict.update({\n",
    "                self.y_ph: y,\n",
    "                self.lr_ph: self.learning_rate,\n",
    "                self.keep_prob_ph: self.keep_prob,\n",
    "                self.is_train_ph: True,\n",
    "                self.teacher_forcing_rate_ph: self.teacher_forcing_rate,\n",
    "            })\n",
    "        return feed_dict\n",
    "    \n",
    "    def train_on_batch(self, x, y):\n",
    "        feed_dict = self._build_feed_dict(x, y)\n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        feed_dict = self._build_feed_dict(x)\n",
    "        y_pred = self.sess.run(self.y_pred_tokens, feed_dict=feed_dict)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用随机权重和默认参数创建模型，更改model的路径，否则它将存储在deeppavlov/download文件夹中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s = Seq2Seq(\n",
    "    save_path='PATH_TO_YOUR_WORKING_DIR/model',\n",
    "    load_path='PATH_TO_YOUR_WORKING_DIR/model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们首先运行所有预处理步骤，然后调用seq2seq模型，然后将token索引转换为token。因此，我们可以得到一些随机的单词序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制\n",
    "注意力机制（论文链接：https://arxiv.org/abs/1409.0473) 可以根据当前状态从“记忆”中收集信息，通过聚合，我们假设出“记忆”项的加权和，每个记忆项的权重取决于当前状态。  \n",
    "  \n",
    "在没有注意力的情况下，解码器只能使用编码器的最后一个隐藏状态，注意力机制允许在解码过程中访问所有编码器状态。  \n",
    "  \n",
    "![](https://github.com/deepmipt/DeepPavlov/raw/c7896c6db96f43f57cacd9a6a471e37cb70bf07a/examples/tutorials/img/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算注意力权值(a_ij)最简单的方法之一是，通过记忆项和状态之间的点积计算它们，然后应用softmax函数。 \n",
    "还有其他计算乘法注意力权值的方法（论文链接：https://arxiv.org/abs/1508.04025） 。  \n",
    "  \n",
    "我们还需要一个掩码来跳过一些序列元素，比如< PAD >,为了使不需要的记忆项的权重接近于零，我们可以在应用softmax函数之前向logits(点积的结果)添加大的负值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_mask(values, mask):\n",
    "    # adds big negative to masked values\n",
    "    INF = 1e30\n",
    "    return -INF * (1 - tf.cast(mask, tf.float32)) + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_attention(memory, state, mask, scope=\"dot_attention\"):\n",
    "    # inputs: bs x seq_len x hidden_dim\n",
    "    # state: bs x hidden_dim\n",
    "    # mask: bs x seq_len\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # dot product between each item in memory and state\n",
    "        logits = tf.matmul(memory, tf.expand_dims(state, axis=1), transpose_b=True)\n",
    "        logits = tf.squeeze(logits, [2])\n",
    "        \n",
    "        # apply mask to logits\n",
    "        logits = softmax_mask(logits, mask)\n",
    "        \n",
    "        # apply softmax to logits\n",
    "        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n",
    "        \n",
    "        # compute weighted sum of items in memory\n",
    "        att = tf.reduce_sum(att_weights * memory, axis=1)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查你的实现:  \n",
    "输出应该是 32 x 100 的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "memory = tf.random_normal(shape=[32, 10, 100]) # bs x seq_len x hidden_dim\n",
    "state = tf.random_normal(shape=[32, 100]) # bs x hidden_dim\n",
    "mask = tf.cast(tf.random_normal(shape=[32, 10]), tf.int32) # bs x seq_len\n",
    "dot_attention(memory, state, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教师强制算法\n",
    "我们已经实现了解码器，在训练和推理过程中，解码器将自己的输出作为输入。但是，在训练的早期阶段，模型很难产生长序列，这取决于它自己是否接近随机输出。教师强迫算法可以解决这个问题:代替进给模型的输出，我们可以输入真实的token。它有助于建模的训练时间，但根据推断，我们仍然只能依赖于它自己的输出。  \n",
    "  \n",
    "使用模型的输出:  \n",
    "< img src=\"img/sampling.png\" alt=\"sampling\" width=50% />  \n",
    "  \n",
    "教师强制算法：  \n",
    "< img src=\"img/teacher_forcing.png\" alt=\"teacher_forcing\" width=50% />  \n",
    "  \n",
    "没有必要在每次步骤中都输入真实的token-如果我们想要真值输入或通过模型预测，我们可以以某个速率随机选择。seq2seq 模型的 teacher_forcing_rate 参数可以控制这种行为。  \n",
    "  \n",
    "更多关于教师强制算法的细节可以在 DeepLearningBook 10.2.1章（链接：http://www.deeplearningbook.org/contents/rnn.html ）中找到  \n",
    "  \n",
    "让我们用随机权重和默认参数创建模型:  \n",
    "  \n",
    "这里，我们首先运行所有预处理步骤，调用seq2seq模型，然后将token索引转换为token。因此，我们应该得到一些随机的单词序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后置处理  \n",
    "在后置处理步骤中，我们将删除所有< PAD >, < BOS >, < EOS >标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('postprocessing')\n",
    "class SentencePostprocessor(Component):\n",
    "    def __init__(self, pad_token='<PAD>', start_token='<BOS>', end_token='<EOS>', *args, **kwargs):\n",
    "        self.pad_token = pad_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = ' '.join(self._postproc(batch[i]))\n",
    "        return batch\n",
    "    \n",
    "    def _postproc(self, utt):\n",
    "        if self.end_token in utt:\n",
    "            utt = utt[:utt.index(self.end_token)]\n",
    "        return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess = SentencePostprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess(vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建配置文件\n",
    "让我们把它们放在一个配置文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"name\": \"personachat_dataset_reader\",\n",
    "    \"data_path\": \"YOUR_PATH_TO_FOLDER_WITH_PERSONACHAT_DATASET\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"name\": \"personachat_iterator\",\n",
    "    \"seed\": 1337,\n",
    "    \"shuffle\": True\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\"x\"],\n",
    "    \"in_y\": [\"y\"],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"x\"],\n",
    "        \"out\": [\"x_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"y\"],\n",
    "        \"out\": [\"y_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"dialog_vocab\",\n",
    "        \"id\": \"vocab\",\n",
    "        \"save_path\": \"YOUR_PATH_TO_WORKING_DIR/vocab.dict\",\n",
    "        \"load_path\": \"YOUR_PATH_TO_WORKING_DIR/vocab.dict\",\n",
    "        \"min_freq\": 2,\n",
    "        \"special_tokens\": [\"<PAD>\",\"<BOS>\", \"<EOS>\", \"<UNK>\"],\n",
    "        \"unk_token\": \"<UNK>\",\n",
    "        \"fit_on\": [\"x_tokens\", \"y_tokens\"],\n",
    "        \"in\": [\"x_tokens\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_tokens\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"sentence_padder\",\n",
    "        \"id\": \"padder\",\n",
    "        \"length_limit\": 20,\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"padder\",\n",
    "        \"in\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"seq2seq\",\n",
    "        \"id\": \"s2s\",\n",
    "        \"max_length\": \"#padder.length_limit+2\",\n",
    "        \"cell_size\": 250,\n",
    "        \"embeddings_dim\": 50,\n",
    "        \"vocab_size\": 11595,\n",
    "        \"keep_prob\": 0.8,\n",
    "        \"learning_rate\": 3e-04,\n",
    "        \"teacher_forcing_rate\": 0.0,\n",
    "        \"use_attention\": False,\n",
    "        \"save_path\": \"YOUR_PATH_TO_WORKING_DIR/model\",\n",
    "        \"load_path\": \"YOUR_PATH_TO_WORKING_DIR/model\",\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"in_y\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens_ids\"],\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_predicted_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"postprocessing\",\n",
    "        \"in\": [\"y_predicted_tokens\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\"y_predicted_tokens\"]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"log_every_n_batches\": 100,\n",
    "    \"val_every_n_epochs\":0,\n",
    "    \"batch_size\": 64,\n",
    "    \"validation_patience\": 0,\n",
    "    \"epochs\": 20,\n",
    "    \"metrics\": [\"bleu\"],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用配置与模型交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "model = build_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(['Hi, how are you?', 'Any ideas my dear friend?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "在有和没有注意力机制的情况下，在教师强迫算法和没有教师强迫算法的情况下进行实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import train_evaluate_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(config, open('seq2seq.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_model_from_config('seq2seq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_from_config(config)\n",
    "model(['hi, how are you?', 'any ideas my dear friend?', 'okay, i agree with you', 'good bye!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了改进模型，您可以尝试使用多层(使用MultiRNNCell)编码器和解码器，尝试使用可训练参数的注意力机制(而不是点积计分函数)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
